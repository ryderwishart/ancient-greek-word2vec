{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32a5c902",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fefcaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import time\n",
    "try:\n",
    "    from collections.abc import Mapping\n",
    "    from gensim.models.word2vec import Word2Vec\n",
    "except:\n",
    "#     print(\"Depencies not found. Make sure you have installed GenSim.\")\n",
    "    !pip install -I gensim\n",
    "#     !pip install -Iv gensim==3.2.0\n",
    "    from collections.abc import Mapping\n",
    "    from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e00a44f",
   "metadata": {},
   "source": [
    "## Locate corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0423832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_directory = './data/corpus'\n",
    "force_lowercase = True\n",
    "use_lemma_disambiguation = False # Some lemmas are indicated with a numeric suffix (e.g., 'ὅτι2')\n",
    "\n",
    "# Change to the directory entered (this is necessary to use the codecs.open() method). \n",
    "# TODO: rewrite this corpus iterator without the codecs module.\n",
    "\n",
    "if not(os.getcwd().split('/')[-1].endswith('corpus')):\n",
    "    os.chdir(corpus_directory)\n",
    "\n",
    "# This class streams through the corpus when called.\n",
    "\n",
    "def tokenize(string):\n",
    "    output = string\n",
    "    if use_lemma_disambiguation:\n",
    "        pass\n",
    "    else:\n",
    "        # Filter numeric digits from token\n",
    "        output = ''.join(filter(lambda x: not x.isdigit(), string))\n",
    "    if force_lowercase:\n",
    "        return [token.lower() for token in output.split()]\n",
    "    else:\n",
    "        return output.split()\n",
    "    \n",
    "class MySentences(object):\n",
    "    def __iter__(self):\n",
    "        for file in os.listdir('.'): # the directory where the text files are.\n",
    "            if file.endswith(\".txt\"):\n",
    "                for line in codecs.open(file, 'r+'):\n",
    "                    tokens = tokenize(line)\n",
    "                    if len(tokens) > 0:\n",
    "                        yield tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c16c57",
   "metadata": {},
   "source": [
    "## Instantiate the corpus streamer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d93a0de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = MySentences()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a703d78",
   "metadata": {},
   "source": [
    "## Sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3d12cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ἐν', 'ὁ', 'πρότερος', 'ὅτι', 'εὔχομαι', 'νύξ', 'καί', 'ἡμέρα', 'ὁράω', 'καί', 'ὅτι', 'οὐ', 'στέγω', 'ἀλλά', 'ἐν', 'ἀθήνη', 'μόνος', 'καί', 'ὅτι', 'πέμπω', 'τιμόθεος', 'διά', 'πᾶς', 'οὗτος', 'ὁ', 'πόθος', 'αὐτός', 'δηλόω', 'ὅς', 'ὥστε', 'παραγίγνομαι', 'πρός', 'αὐτός']\n",
      "['ἐπεί', 'οὖν', 'οὐ', 'φθάνω', 'ἴσος', 'ἀπέρχομαι', 'καί', 'καταρτίζω', 'ὁ', 'ὑστέρημα', 'ὁ', 'πίστις', 'αὐτός', 'οὗτος', 'χάρις', 'ὁ', 'δεύτερος', 'προστίθημι']\n",
      "['ὁ', 'ἐλλείπω', 'ἀπό', 'ὁ', 'παρουσία', 'διά', 'ὁ', 'πρᾶγμα']\n",
      "['ὅτι', 'γάρ', 'οὐ', 'ἀπέρχομαι', 'εἰμί', 'στοχάζομαι']\n",
      "['γράφω', 'γάρ', 'φημί', 'ἐρωτάω', 'δέ', 'ὑπέρ', 'ὁ', 'παρουσία', 'ὁ', 'ἐγώ', 'ἰησοῦς']\n",
      "['καί', 'γάρ', 'ἐν', 'ὁ', 'πρότερος', 'στολή', 'λέγω', 'ὅτι', 'περί', 'δέ', 'ὁ', 'χρόνος', 'καί', 'καιρός', 'οὐ', 'χρεία', 'ἔχω', 'γράφω', 'ὥστε', 'εἰ', 'παραγίγνομαι', 'δέω', 'γράφω']\n"
     ]
    }
   ],
   "source": [
    "# Output should resemble: ['ἐν', 'ὁ', 'πρότερος', 'ὅτι2', 'εὔχομαι', 'νύξ', 'καί', 'ἡμέρα', 'ὁράω', ...etc.\n",
    "count = 0\n",
    "for i in sentences:\n",
    "    count += 1\n",
    "    print(i)\n",
    "    if count > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2b3de0",
   "metadata": {},
   "source": [
    "## Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25a4c009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector size (too small = underfit; too large = overfit)\n",
    "size_input = 300 \n",
    "\n",
    "# Window size (small = paradigmatic model; large = syntagmatic model)\n",
    "window_input = 5 \n",
    "\n",
    "# Minimum word count for inclusion in network\n",
    "min_count_input = 2 # If a word occurs few times then its vector will not be very high quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a086553",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "682c49fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model . . . \n",
      "\n",
      "Model initialized in 96.59753274917603 seconds.\n",
      "\n",
      "Total number of unique words in corpus: 31754.\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating model . . . \")\n",
    "start = time.time()\n",
    "model = Word2Vec(sentences, vector_size=size_input, window=window_input, min_count=min_count_input, workers=4)\n",
    "# Create a list of all the unique words in the corpus, in case user wants to query all words.\n",
    "#     words_seen = set() # holds lines already seen\n",
    "#     allWords = []\n",
    "#     for line in sentences:\n",
    "#         for word in line:\n",
    "#             if word not in words_seen: # not a duplicate\n",
    "#                 allWords.append(word)\n",
    "#                 words_seen.add(word)   \n",
    "\n",
    "print(\"\\nModel initialized in {0} seconds.\".format(time.time() - start))\n",
    "print(\"\\nTotal number of unique words in corpus: {0}.\".format(len(model.wv)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7478c819",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "12b26fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'nov2022'\n",
    "\n",
    "model.save(f'../../models/{model_name}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3056306",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
